---
title: "Online Reputation"
subtitle: "Social Media and Web Analytics"
author: "Lachlan Deer"
institute: "Tilburg University"
date: "Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, metropolis, metropolis-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: font200

# Learning Goals for this Week

At the end of this lecture you will be able to:

1. Summarize the impact of negative reputation on sales
2. Diagnose situations where fake reviews are more or less prevalent
3. Explain how managerial responses to online reviews impact future reviews
4. Interpret regression estimates from existing studies

---
class: inverse, center, middle

# Online Reputation Matters

---
# Online Reputation Matters

```{r, echo = FALSE, fig.align = "center", out.width="65%"}
knitr::include_graphics("figs/reputation_matters.png")
```

Image from [WebpageFX](https://www.webpagefx.com/)

---
# Online Reputation Effects Perceptions

```{r, echo = FALSE, fig.align = "center", out.width="70%"}
knitr::include_graphics("figs/reputation_perceptions.png")
```

Image from [WebpageFX](https://www.webpagefx.com/)

---
# Online Reputation Effects Sales

```{r, echo = FALSE, fig.align = "center", out.width="80%"}
knitr::include_graphics("figs/reputation_sales.png")
```

Image from [WebpageFX](https://www.webpagefx.com/)

---
class: font160
# Why Online Reputation Matters

Buyers need to trust sellers

* Product descriptions
* Fulfilling transactions 

Sellers need to trust buyers 

* Ensure buyer will pay
* Abide by terms of service

Where does this trust come from?

$\implies$ **reputation systems** 

... and design choices made by a marketplace

---
class: font160
# What do we want to know?

This class:

* How does seller reputation impact pricing and sales?
* Do fake reviews impact online reputation? How? When?
* Are managerial responses an effective way to manage online reputation?

**Note**: There's much more out there - let us know if you want further links in to the literature

---
class: inverse, center, middle

# Dynamics of Seller Reputation

---
class: font160
# Seller Reputation & Online Marketplaces 

**Motivation**: Reputation mechanisms allow consumers to monitor firms

* How do consumers respond to changes in seller reputation?

**Specific Questions**: What is the effect of reputation on:

* Price / Willingness to Pay
* Sales Growth
* Subsequent reviews

Following discussion based on [Cabral & Hortacsu, 2010][cabral]

[cabral]: https://onlinelibrary.wiley.com/doi/10.1111/j.1467-6451.2010.00405.x

---
class: font160
# Empirical Approach

**Data**: eBay, follow sellers of five homogeneous products

* Transaction level data 
* Seller's sequence of reviews

**Empirical Approach**:

* Descriptive Regressions
* Differences in Means

**None** of the effects we discuss here are **causal**

* Think of the results as descriptive associations between two variables

---
# Reputation & Price

**Estimating Equation**:

$$
price = \beta ( \text{reputation_measure} ) +
        \gamma( \text{other demand factors} ) +
        \text{error}
$$

**Excerpt from Table 2**:

```{r, echo = FALSE, fig.align = "center", out.width="90%"}
knitr::include_graphics("figs/ebay_price.png")
```

---
class: font160
# Negative Feedback & Sales Growth

**Metric**: Difference in Sales Growth before / after first negative feedback

```{r, echo = FALSE, fig.align = "center", out.width="90%"}
knitr::include_graphics("figs/ebay_salesgrowth.png")
```

---
class: font160
# Frequency of Negative Feedback

**Metric**: Frequency of Arrival of Negative Feedback

```{r, echo = FALSE, fig.align = "center", out.width="90%"}
knitr::include_graphics("figs/ebay_commentfreq.png")
```

---
class: font160
# Managerial Implications

1. **Price / Willingness to Pay** is **not the main metric** through which decreases in reputation operates
2. **Quantity sold** is an **important** metric
  * Sales decrease with negative feedback 
3. **Negative Feedback can generate more negative feedback**
  * Though authors argue this is moral hazard - less effort by sellers

---
class: inverse, center, middle

# Online Review Manipulation

---
class: font160
# Reputation Manipulation 

**Motivation**: Reputation is most useful when it's not tainted by "fake reviews"

* Fake reviews lead to:
  * Lower consumer welfare through sub-optimal choices 
  * Mistrust in online reviews and reputation

**Question**: When does review manipulation occur?

* Are there more fake reviews when competition is close by?
* Do smaller hotels try to boost their reputation?
  * More positive fake reviews for small hotels?
  * More negative reviews for competitor nearby a small hotel?

Following discussion based on [Mayzlin, Dover and Chevalier, 2014][mayzlin]

[mayzlin]: https://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.104.8.2421

---
class: font140
# Empirical Approach

**Data**: Travel sites in the US - TripAdvisor & Expedia
  * Star Ratings of all reviews for all hotels in subset of cities in the US
  * Supplement with hotel industry data from Smith Travel Research

**Empirical Approach**: Linear Regression

* Authors argue its some kind of difference in difference regression
  * We'll cover DiD next week in labs
* This paper is not DiD in a 'standard sense'

**What makes all this work**:
  * TripAdvisor: Anyone can post at anytime
  * Expedia: Can only post if booked on Expedia and stayed one night in last 6 months
  * $\implies$ fake reviews are harder to post on Expedia
  * **Assumption**: Users on each platform value hotel characteristics equally

---
class: font130
# Regression Equation

Estimate the following equation:

$$
\begin{equation}
y_{ij} = X_{ij} B_1 + 
   \text{OwnAf}_{ij} B_2 +
   \text{Nei}_{ij} B_3+
   \text{NeiOwnAf}_{ij} B_4 +
   \sum \gamma_j + 
   \varepsilon_{ij}
\end{equation}
$$

Notation:

* $i$ hotels, $j$ city
* $y_{ij}$ Difference in share of $N$ star reviews between TripAdvisor and Expedia 
* $X_{ij}$ are hotel characteristics
* $\text{Nei}_{ij}$ = 1 if competitor hotel within 0.5 km, else zero
  * We care about these coefficients, $B_3$
* $\text{OwnAf}_{ij}$ are hotel ownership characteristics
  * We care about these coefficients, $B_2$
* $\text{NeiOwnAf}_{ij}$ are competitor hotels ownership characteristics
  * We care about these coefficients, $B_4$

---
class: font160
# Why this approach will work ...

Authors don't observe review manipulation directly $\implies$ infer it from data patterns

* It's easier to manipulate reviews on TripAdvisor...

The story goes something like this:

* If the fraction of low (high) reviews on TripAdvisor is larger than on Expedia 
* And consumers value the hotel equally between platforms 
* Then differences are likely due to review manipulation on TripAdvisor 

So let's check out the results...

---
# Main Results

```{r, echo = FALSE, fig.align = "center", out.width="67%"}
knitr::include_graphics("figs/fake_main_results.png")
```

---
# Interpreting Results

Column 1:

* $B_3$: 0.0192 $\implies$ **hotels with a neighbouring competitor have a 1.9 percentage point increase in share of bad reviews**
  * approx. 7.5 percent increase compared to the baseline of 25 percent bad reviews

Column 2: 

* $B_3 + B_4$: $\implies$ **hotels with an independent hotel as a  neighbouring competitor have a 4.7 percentage point increase in share of bad reviews**
  * approx. 20 percent increase compared to the baseline of 25 percent bad reviews  

Column 3: 

* $B_2$: $\implies$  **independent hotels have a 3.4 percentage point increase in share of positive reviews**
  * approx. 7.5 percent increase compared to the baseline of 31 percent five star reviews

---
# Takeaways

1. **Hotels with neighbors have more negative reviews**
  * Suggestive of competitors giving each other negative fake reviews

2. If **neighbor is an independent** hotel, (1) is even **more likely** 

3. Independent hotels have higher reviews
  * Suggestive of positive review manipulation
  * But there are competing stories

**Punchline**: Evidence for fake reviews and manipulating online reputation
  * Either by competitors (negative) or by the firm itself (positive)

**Managerial Implications?**

* More for platform owners ...
* There's a need to try and monitor / control reviews

---
class: inverse, center, middle

# Managerial Response to Online Reviews

---
class: font160
# Managerial Reviews & Reputation 

**Motivation**: Business increasingly proactive to managing reputation

**One Approach**: Managerial Responses

**Question**: What is the effectiveness of Managerial Responses on future reviews?

* Are there more or less? $\implies$ volume effects
* Are the more or less positive $\implies$ valence effects

Following discussion based on [Chen et al, 2019][chen-etal]

[chen-etal]: https://repository.arizona.edu/bitstream/handle/10150/632181/2%20ISR-MR-Paper.pdf?sequence=1

---
class: font160
# Empirical Approach

**Data**: Travel Agencies in China (two): Ctrip & eLong

**Empirical Approach**: Linear Regression

* They implement what is known as a "difference in the difference of differences" regression
  * That will make more sense next week, after we formally talk about DiD
  * We can interpret the results through what we already know about linear regression though!

---
class: font160
# Why this works ...

Ctrip introduces managerial response, eLong does not

* Intuitively: comparing differences in reviews between platforms before and after managerial responses are introduced
  * If there's a change - its due to the introduction of MR
  * That's the "difference in difference" part

* Extra layer of concern: hotels choose whether to adopt managerial response
  * So it's not "random"
  * Trying to control for that is where the "extra difference" comes in
  * (Though I am slightly skeptical...)

<!-- --- -->
<!-- # Difference-in-Difference-in-Differences -->

<!-- Why an extra difference? Let's think intuitively... -->

<!-- * Assume only two periods, 1 and 2 -->
<!-- * Ctrip ($C$) allows managerial response in period 2, eLong ($E$) does not -->
<!-- * $\bar{y}_{i,j}$ = average number of reviews on platform $i$ in period $j$ -->

<!-- **Difference in Differences**: -->

<!-- * Difference in average volume between 2 periods across different platforms: -->

<!-- $$ -->
<!-- \begin{equation} -->
<!-- \hat{\beta}_{DiD} =  (\bar{y}_{C,2} - \bar{y}_{C,1}) - (\bar{y}_{E,2} - \bar{y}_{E,1}) -->
<!-- \end{equation} -->
<!-- $$ -->

<!-- * Assumes that hotels that decide to use managerial response on Ctrip are not different to those who don't adopt it -->
<!--   * "No inherent (managerial) quality difference" -->
<!--   * If there is, it will make estimates of the treatment effect **biased** -->

<!-- Why are we worrying about this? -->

<!-- * Hotels can self select into whether they adopted Managerial Responses on Ctrip -->
<!--   * And those that do might be systematically different than non-adopters -->
<!--   * Self selection is *always* a worry when we want causality -->

<!-- --- -->
<!-- # Difference-in-Difference-in-Differences -->

<!-- * Some more notation: -->
<!--   * Let $A$ denote hotels that adopt managerial responses on Ctrip -->
<!--   * Let $N$ denote hotels that do not adopt managerial responses on Ctrip -->

<!-- *Note*: We observe reviews of both types of hotels on both platforms! -->

<!-- **Difference-in-Difference-in-Differences** -->

<!-- * Difference in differences of average volumes between 2 periods and each hotel "type" across different platforms -->

<!-- $$ -->
<!-- \begin{align} -->
<!-- \hat{\beta}_{DDD} &= [ (\bar{y}_{C,A,2} - \bar{y}_{C,A,1}) - (\bar{y}_{C,N,2} - \bar{y}_{C,N,1})] \\ -->
<!--      &- [ (\bar{y}_{E,A,2} - \bar{y}_{E,A,1}) - (\bar{y}_{E,N,2} - \bar{y}_{E,N,1})] -->
<!-- \end{align} -->
<!-- $$ -->

<!-- We've "subtracted out" the differences in managerial quality -->

---
# Regression Equation

How do the authors do that as a regression?

$$
\begin{equation}
\Delta Y_{it} = 
   \gamma MR_i +
   \beta MR_{i} \times After_{it} + 
   \delta ' X_{it} + 
   \alpha_i + 
   \theta_t +
   \varepsilon_{it}
\end{equation}
$$

* $\beta$ is the treatment effect $\implies$ this is the (only) number we care about in this regression

Some notation:

* $i$ is a hotel, $t$ is time
* $\Delta Y_{it}$ difference in review volume (valence) between Ctrip and eLong
* $MR_i$ has hotel done any managerial response on Ctrip
  * Binary variable -- 0 or 1
* $After_{it}$ tells us whether managerial response feature has been "turned on" in the Ctrip Platform
  * Binary variable -- 0 or 1

$\implies$ $\beta$ is the average effect of managerial responses on the difference in review volume (valence) between platforms

---
class: font130
# Main Results - Volume, not Valence

```{r, echo = FALSE, fig.align = "center", out.width="80%"}
knitr::include_graphics("figs/mr_main_result.png")
```

Interpretation:

* Specification (3) and (6) are the richest
* On average, 12.3% increase in monthly volume after adopting managerial responses 

---
class: font130
# Target & Style of Managerial Response

```{r, echo = FALSE, fig.align = "center", out.width="65%"}
knitr::include_graphics("figs/mr_length_result.png")
```

Main Takeaways:

* Short Responses to Positive Reviews to not distract consumers
* Longer Responses to Negative Reviews to mitigate concerns 

---
class: inverse, center, middle

# Recap

---
class: font160
# Summary

* Online reputation matters --- suggestive evidence that decreasing reputation decreases sales

* Competitors seem to use online platforms to post negative fake reviews about each other
  * And they might provide positive fake reviews about themselves

* Managing reputation via responses to comments on large platforms stimulates more volume 
  * Does it effect reputation though?
  * Group Assignment 1 will try and tackle this question!

---
# License & Citation

Suggested Citation:

```{r, engine='out', eval = FALSE}
@misc{smwa-lecture-02,
      title={"Social Media and Web Analytics: Lecture 02 - Online Reputation"},
      author={Lachlan Deer},
      year={2021},
      url = "https://github.com/tisem-digital-marketing/smwa-lecture-02"
}
```

<p style="text-align:center;"><img src="https://www.tilburguniversity.edu/sites/default/files/styles/large_width/public/image/Logo%20OSCT.png?itok=PqU9mw_l" alt="Logo" width = "200"></p>

This course adheres to the principles of the Open Science Community of Tilburg University. 
This initiative advocates for transparency and accessibility in research and teaching to all levels of society and thus creating more accountability and impact.

<p style="text-align:center;"><img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" alt="Logo" width = "100"></p>
This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.